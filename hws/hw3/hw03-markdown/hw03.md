
# <center>Statistical Models and Computing Methods, Problem Set 3
<center>王宇哲 2201112023
<center>Academy for Advanced Interdisciplinary Studies, Peking University



## Problem 1

A total of $n$ instruments are used to observe the same astronomical source. Suppose the number of photons recorded by instrument $j$ can be modeled as $y_j \sim {\rm Poisson}(x_j\theta + r_j)$ where $\theta\geq 0$ is the parameter of interest, and $x_j$ and $r_j$ are known positive constants. You may think of $\theta, x_j , r_j$ as the source intensity, the observation time, and the background intensity for instrument $j$, respectively. Assume the photon counts across different instruments are independent.

(1) Write down the likelihood function for $\theta$.

***Proof.*** Consider that
$$
p(y_i=k|\theta)=\frac{(x_j\theta + r_j)^k e^{-(x_j\theta + r_j)}}{k!}
$$
hence the log-likelihood function for $\theta$ is 
$$
\mathcal{L}(\theta)=\log p(\boldsymbol{y}|\theta)=\sum_{j=1}^{n}\log p(y_j|\theta)=\sum_{j=1}^{n}\Big(y_j\log(x_j\theta+r_j)-(x_j\theta+r_j)-\log y_j!\Big)
$$
in which $\boldsymbol{y}=(y_1,\ldots,y_n)^{T}$

(2) Introduce mutually independent latent variables $z_{j1}\sim {\rm Poisson}(x_j\theta)$ and $z_{j2}\sim{\rm Poisson}(r_j)$ and suppose we observe only $y_j \equiv z_{j1} + z_{j2}$. Under this formulation, derive an EM algorithm to find the MLE of $\theta$.

***Proof.*** Consider that
$$
p(y_j,\boldsymbol{z}_j|\theta)=p(y_j|z_{j1},z_{j2},\theta)p(z_{j1}|\theta)p(z_{j2}|\theta)=\frac{(x_j\theta)^{z_{j1}}r_j^{z_{j2}}e^{-(x_j\theta+r_j)}}{z_{j1}!z_{j2}!}
$$
in which $\boldsymbol{z}_j=(z_{j1},z_{j2})^{T}$. We have the complete data log-likelihood
$$
\mathcal{l}(y_j,\boldsymbol{z}_j|\theta)=\log p(y_j,\boldsymbol{z}_j|\theta)=z_{j1}\log(x_j\theta)+z_{j2}\log r_j-x_j\theta-r_j-\log (z_{j1}!z_{j2}!)
$$
Now we compute
$$
\sum_{\boldsymbol{z}_j}p(y_j,\boldsymbol{z}_j|\theta)=\sum_{k=0}^{\infty}\sum_{l=0}^{\infty}\frac{(x_j\theta)^{k}r_j^{l}e^{-(x_j\theta+r_j)}}{k!\,l!}=1
$$
Hence the conditional probability
$$
p(\boldsymbol{z}_j|y_j,\theta)=\frac{p(y_j,\boldsymbol{z}_j|\theta)}{\sum_{\boldsymbol{z}_j}p(y_j,\boldsymbol{z}_j|\theta)}=p(y_j,\boldsymbol{z}_j|\theta)
$$


Hence the expected complete data log-likelihood at timestep $t$ is
$$
Q^{(t)}(\theta)=\sum_{j=1}^{n}\sum_{\boldsymbol{z}_j}p(\boldsymbol{z}_j|y_j,\theta^{(t)})\mathcal{l}(y_j,\boldsymbol{z}_j|\theta^{(t)})=\sum_{j=1}^{n}\sum_{\boldsymbol{z}_j}p(y_j,\boldsymbol{z}_j|\theta)\log p(y_j,\boldsymbol{z}_j|\theta)
$$


(3) Apply your EM algorithm to the data set given by Table 1. What is the MLE?

<img src="Table1.jpg" style="zoom: 30%;" />

(4) For these data compute the observed Fisher information and the fraction of missing information. (Recall the observed Fisher information is defined as the negative second derivative of the observed data log-likelihood evaluated at the MLE.)

## Problem 2

Let $x_1,\ldots x_m$ be i.i.d. sample from a normal distribution with mean $\mu$ and variance $\sigma^2$. Suppose for each $x_i$ we observe $y_i = |x_i|$ rather than $x_i$. Download the data from the course website.

(1) Derive an EM algorithm to find the MLE of $\mu$ and $\sigma^2$.

(2) Apply your EM algorithm to the data with different starting values. Does your EM always converge to the same point estimate? If not, do you observe any pattern of your estimates? Explain it.

(3) Derive the gradient of the parameters. Compare the standard gradient descent method to EM. Show $\mathcal{l}^*-\mathcal{l}$ as a function of the number of iterations ($\mathcal{l}$ is the log-likelihood function and $\mathcal{l}^*$ is the optimal value of it) for both methods. Which one is better in this case? Why?

## Problem 3

In this problem, we will apply LDA to human ancestry discovery. In applications of population genetics, it is often useful to classify individuals in a sample into populations. An underlying assumption is that there are $K$ ancestor populations, and each individual
is an admixture of the ancestor populations. For each individual, we measure some genetic data about them, called genotype data. Each genotype is a locus that can take a discrete count value, individuals with similar genotypes are expected to belong to the
same ancestor populations. We can derive the admixture coefficients $\theta$ for each individual by running an LDA model, where the documents are individuals, and the words are the genotype.

Now let us assume the $\beta$ matrix is known, and focus on variational inference of the population mixture $\theta$ and the genotype ancestry (topic) assignments $z$ for any individual. The variational distribution used to approximate the posterior (for each individual) is
$$
q_i(\theta,z|\gamma,\phi)=q(\theta_i|\gamma_i)\prod_{n=1}^{N_i}q(z_{in}|\phi_{in}),\ \ i=1,\ldots,M
$$
The data matrix provides data about $M = 100$ individuals, each represented by a vocabulary of $N = 200$ genotype loci. This data has been preprocessed into a count matrix $D$ of size $M \times N$. $D_{ij}$ is the number of occurrences of genotype $j$ in individual $i$, and $\sum_j D_{ij}$ is the number of genotype loci in an individual. We learnt the LDA topic model over $K = 4$ ancestor populations, and the data matrix and the known matrix can be downloaded from the course website. The value of $\alpha$ is $0.1$. You may use the following code to load the data in python.

(1) Derive the variational inference update equations for estimating $\gamma$ and $\phi$.

(2) For individual one, run LDA inference to find $\phi$ for each genotype locus, store it as a matrix of size $n_1 \times K$ (where $n_1$ : $\sum_{1j} I(D_{1j} \neq 0)$, $I(\cdot)$ being the indicator function, is the number of non-zero genotypes present in individual 1), and plot it as an image in your write up. Don't forget to show the colormap using the colorbar function to allow the colors in the image to be mapped to numbers!

(3) We will construct a matrix $\Theta$ of size $M \times K$ to represent the ancestor assignments for all individuals in the population. For each individual $i$, run LDA inference to find $\gamma$, and store it as row of $\Theta$, i.e. $\Theta_i = \gamma$. Visualize $\Theta$ as an image.

(4) Report the number of iterations needed to get to convergence for running inference on all $M$ individuals (you may use absolute change less than $1e\text{-}3$ as the convergence criteria).

(5) Repeat the experiment for $\alpha = 0.01$, $\alpha = 1$, $\alpha = 10$, and for each of $\alpha$, visualize the $\Theta$ matrix summarizing the ancestor population assignments for all individuals. Discuss the changes in the ancestor population assignments to the individuals as $\alpha$ changes. Does the mean number of iterations required for convergence for inference change as $\alpha$changes?







